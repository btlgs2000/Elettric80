{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_cartpole_from_tuple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1gk3Bq1qWfWh8mKL8y/78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btlgs2000/Elettric80/blob/main/DQN_cartpole_from_tuple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQnoF5Ud2hqb"
      },
      "source": [
        "import gym\n",
        "import tensorflow.keras as keras\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m1vH2_-wQTs"
      },
      "source": [
        "from keras import Sequential, Input\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtyxEoLx2i6T"
      },
      "source": [
        "model = Sequential(\n",
        "    [\n",
        "     Input(shape=(4,)),\n",
        "     Dense(100, 'relu'),\n",
        "     Dense(100, 'relu'),\n",
        "     Dense(100, 'relu'),\n",
        "     Dense(2)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8xzDEPVw7O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bb7e36-bbf3-4935-acfe-beb463a8d899"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 100)               500       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 20,902\n",
            "Trainable params: 20,902\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO17gEWiv6RW"
      },
      "source": [
        "# Esempio di generazione di un episodio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_tymRv8u15c"
      },
      "source": [
        "cart_pole = gym.make('CartPole-v0')\n",
        "obs = cart_pole.reset()\n",
        "done = False\n",
        "counter = 0\n",
        "while not done:\n",
        "    obs, rew, done, info = cart_pole.step(cart_pole.action_space.sample())\n",
        "    counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gSbyqZf7ssV"
      },
      "source": [
        "NUM_EPISODES = 1_000\n",
        "REPLAY_BUFFER_CAPACITY = 10_000\n",
        "MIN_REPLAY_BUFFER_CAPACITY = 1_000\n",
        "TARGET_MODEL_UPDATE_PERIOD = 1_000\n",
        "BATCH_SIZE = 32\n",
        "UPDATE_EVERY_N_STEPS = 10\n",
        "# epsilon decade esponenzialmente dal valore iniziale a quello finale\n",
        "# in NUM_EPISODES\n",
        "EPSILON_0 = 0.9\n",
        "EPSILON_N = 0.1\n",
        "LR = 1e-3\n",
        "GAMMA = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnujcQH3Xk8X"
      },
      "source": [
        "def choose_action(observation, epsilon, model):\n",
        "    ''' sceglie un'azione secondo il criterio epsilon-greedy'''\n",
        "    if random.random() < epsilon:\n",
        "        # azione casuale\n",
        "        action = random.randint(0, 1)\n",
        "    else:\n",
        "        # azione migliore\n",
        "        action = np.argmax(model(np.expand_dims(observation, axis=0)))\n",
        "    \n",
        "    return action\n",
        "\n",
        "def get_target_value(reward, next_state, target_model, gamma):\n",
        "    ''' Valore target calcolato in base al reward ottenuto\n",
        "    e allo stato successivo incontrato'''\n",
        "    if next_state is None:\n",
        "        return reward\n",
        "    else:\n",
        "        return reward + gamma*max(target_model(next_state))\n",
        "\n",
        "def clone_model(model):\n",
        "    ''' Clona un modello e i suoi pesi'''\n",
        "    cloned = keras.models.clone_model(model)\n",
        "    cloned.set_weights(model.get_weights())\n",
        "    return cloned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y77MTE8Yw2I8"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=LR)\n",
        "loss = keras.losses.Huber()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXaRpMuJvF-5",
        "collapsed": true
      },
      "source": [
        "# copia il modello, compresi i pesi\n",
        "target_model = clone_model(model)\n",
        "\n",
        "# inizializza replay buffer\n",
        "observations = deque(maxlen=REPLAY_BUFFER_CAPACITY)\n",
        "actions = deque(maxlen=REPLAY_BUFFER_CAPACITY)\n",
        "next_observations = deque(maxlen=REPLAY_BUFFER_CAPACITY)\n",
        "rewards = deque(maxlen=REPLAY_BUFFER_CAPACITY)\n",
        "\n",
        "global_step_counter = 0\n",
        "\n",
        "with tqdm(total=NUM_EPISODES) as tqdm_bar:\n",
        "    # for num_episode, epsilon in zip(range(1, NUM_EPISODES+1), np.logspace(np.log10(EPSILON_0), np.log10(EPSILON_N), NUM_EPISODES)):\n",
        "    for num_episode, epsilon in zip(range(1, NUM_EPISODES+1), np.linspace(EPSILON_0, EPSILON_N, NUM_EPISODES)):\n",
        "        # genera un episodio\n",
        "        obs = cart_pole.reset()\n",
        "        done = False\n",
        "        episode_counter = 0\n",
        "        while not done:\n",
        "            # sceglie l'azione secondo il modello epsilon-greedy\n",
        "            action = choose_action(obs, epsilon, model)\n",
        "\n",
        "            # esegue l'azione\n",
        "            next_obs, rew, done, info = cart_pole.step(action)\n",
        "\n",
        "            # appends\n",
        "            observations.append(obs)\n",
        "            actions.append(action)\n",
        "            next_observations.append(next_obs if not done else None)\n",
        "            rewards.append(rew)\n",
        "\n",
        "            # info update\n",
        "            obs = next_obs\n",
        "            episode_counter += 1\n",
        "            global_step_counter += 1\n",
        "\n",
        "            # update della rete\n",
        "            # ogni UPDATE_EVERY_N_STEPS step se global_step_counter > MIN_REPLAY_BUFFER_CAPACITY\n",
        "            if (global_step_counter > MIN_REPLAY_BUFFER_CAPACITY) and (global_step_counter % UPDATE_EVERY_N_STEPS == 0):\n",
        "                # compone il batch\n",
        "                # sceglie BATCH_SIZE elementi dal buffer di replay\n",
        "                batch_idxs = random.sample(range(len(observations)), BATCH_SIZE)\n",
        "\n",
        "                # target\n",
        "                batch_observations = []\n",
        "                batch_targets = []\n",
        "                for idx in batch_idxs:\n",
        "                    obs = observations[idx]\n",
        "                    act = actions[idx]\n",
        "                    rew = rewards[idx]\n",
        "                    obs1 = next_observations[idx]\n",
        "\n",
        "                    # valori attuali\n",
        "                    target = model(np.expand_dims(obs, axis=0)).numpy().reshape(-1)\n",
        "\n",
        "                    # modifica il valore dell'azione scelta\n",
        "                    if obs1 is not None:\n",
        "                        target[act] = rew + GAMMA*target_model(np.expand_dims(obs1, axis=0)).numpy().reshape(-1)[act]\n",
        "                    else:\n",
        "                        target[act] = rew\n",
        "\n",
        "                    batch_observations.append(obs)\n",
        "                    batch_targets.append(target)\n",
        "\n",
        "                batch_observations = np.stack(batch_observations)\n",
        "                batch_targets = np.stack(batch_targets)\n",
        "\n",
        "                # esegui uno step dell'ottimizzatore \n",
        "                optimizer.minimize(lambda: loss(batch_targets, model(batch_observations)), model.trainable_weights)\n",
        "            \n",
        "            if global_step_counter % TARGET_MODEL_UPDATE_PERIOD == 0:\n",
        "                # la rete target viene allineata all'altra\n",
        "                target_model.set_weights(model.get_weights())\n",
        "\n",
        "        # episodio finito\n",
        "        tqdm_bar.update()\n",
        "        tqdm_bar.set_postfix(dict(lunhezza_episodio=episode_counter, epsilon=epsilon))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}